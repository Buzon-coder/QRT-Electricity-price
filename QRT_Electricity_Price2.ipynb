{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8gOPU0OBv1tLGO2o2aVf+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Buzon-coder/QRT-Electricity-price/blob/main/QRT_Electricity_Price2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5XqUoAcHeDg",
        "outputId": "eb0776e3-699b-4573-a5bb-f52eec1aeb91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import spearmanr, pearsonr\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "uZwgoIl9Hl4U"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = pd.read_csv(\"/content/drive/MyDrive/Colab data/X_test_final.csv\")\n",
        "X_train = pd.read_csv(\"/content/drive/MyDrive/Colab data/X_train_NHkHMNU.csv\")\n",
        "y_train = pd.read_csv(\"/content/drive/MyDrive/Colab data/y_train_ZAN5mwg.csv\")"
      ],
      "metadata": {
        "id": "LvBtQdxgHrjT"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_id = set(X_train[\"ID\"]) & set(X_test[\"ID\"])\n",
        "print(\"Nombre d'ID' en commun entre TRAIN et TEST:\",len(common_id))\n",
        "\n",
        "common_days = set(X_train[\"DAY_ID\"]) & set(X_test[\"DAY_ID\"])\n",
        "print(\"Nombre de jours en commun entre TRAIN et TEST:\",len(common_days))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbXj7LQiHzD3",
        "outputId": "c4f39b16-e497-4f2a-8198-578cb035932c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre d'ID' en commun entre TRAIN et TEST: 0\n",
            "Nombre de jours en commun entre TRAIN et TEST: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# je trie X_train par DAY_ID croissant (donc par dates) et j'aligne les ID de y_train avec ceux de DAY_ID\n",
        "\n",
        "X_train = X_train.sort_values([\"DAY_ID\", \"COUNTRY\"]).reset_index(drop=True)\n",
        "y_train = y_train.set_index(\"ID\").loc[X_train[\"ID\"]].reset_index()\n",
        "assert all(X_train[\"ID\"].values == y_train[\"ID\"].values)\n",
        "print(\"Les ID sont bien alignés avec X_train\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEdSGopcICIB",
        "outputId": "a3947688-9e2c-4d5e-c960-7ff08562d490"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Les ID sont bien alignés avec X_train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(X_train.head())"
      ],
      "metadata": {
        "id": "AGYiTTUxIE6h"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "J'encode les features de X_train qui ne sont pas des int ou des float (en l'occurence, seul \"COUNTRY\" n'est pas un float ou un int, donc c'est la seule colonne que j'encode)"
      ],
      "metadata": {
        "id": "ZLKQbOI6IPfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##j'encode la colonne COUNTRY en 2 colonnes : COUNTRY_FR et COUNTRY_DE, et je supprime la colonne COUNTRY\n",
        "X_train_encode = X_train.copy()\n",
        "X_train_encode[\"COUNTRY_FR\"] = (X_train_encode[\"COUNTRY\"] == \"FR\").astype(int)\n",
        "X_train_encode[\"COUNTRY_DE\"] = (X_train_encode[\"COUNTRY\"] == \"DE\").astype(int)\n",
        "X_train_encode = X_train_encode.drop(columns = [\"COUNTRY\"])\n",
        "#print(X_train_encode.head())"
      ],
      "metadata": {
        "id": "HMTjXza8IG8N"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On regarde maintenant les corrélations (en valeur absolue) entre les features et les TARGET"
      ],
      "metadata": {
        "id": "-JHaHJ0eI6b6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corrs = X_train_encode.corrwith(y_train[\"TARGET\"], method = 'spearman') # Remarque : la fonction corrwith calcule la corrélation entre les features de X_train et les valeurs TARGET de y_train ligne par ligne\n",
        "                                                                        # Remarque 2 : on utilise la corrélation de Spearman car c'est la corrélation de l'énoncé\n",
        "corrs_abs = corrs.abs().sort_values(ascending=False) # on regarde les valeurs absolues des corrélations des features avec la TARGET. En effet, une corrélation négative mais avec une valeur absolue élevée est aussi une corrélation forte, c'est simplement\n",
        "                                                     # qu'elle agit dans le sens inverse de l'évolution du prix du futures (qd feature augmente, cible diminue, ou l'inverse). exemple : si augmentation du vent, prix diminue car on a + d'électricité\n",
        "#print(corrs_abs.head(20))\n",
        "#print(corrs_abs.tail(18))"
      ],
      "metadata": {
        "id": "U5l3TG9JIhoX"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "J'obtiens que les features les plus corrélées (en valeur absolue) de X_train et avec les TARGET (de y_train) sont par ordre décroissant : DE_NET_IMPORT, DE_NET_EXPORT, DE_WINDPOW, DE_RESIDUAL_LOAD, FR_WINDPOW,DE_HYDRO, DE_GAS, CARBON_RET, DE_WIND, DE_COAL, GAS_RET, FR_HYDRO,FR_WIND, DE_CONSUMPTION, FR_RAIN, FR_DE_EXCHANGE, DE_FR_EXCHANGE, FR_COAL, FR_TEMP, DE_LIGNITE.\n",
        "\n",
        "\n",
        "De même, j'obtiens les features les moins corrélées (en valeur absolue) de X_train avec les TARGET sont (de la plus corrélées à la moins corrélée) : DE_RAIN, DE_SOLAR, FR_SOLAR, FR_CONSUMPTION, ID, COUNTRY_FR, COUNTRY_DE, DE_TEMP, FR_RESIDUAL_LOAD, COAL_RET, FR_GAS, FR_NET_EXPORT, FR_NET_IMPORT, DE_NUCLEAR, DAY_ID, FR_NUCLEAR.\n",
        "\n",
        "Regarder les corrélations permet de comprendre quelles features influencent TARGET, et permet d'interpréter phyisquement le modèle. Ces features représentent les effets dominants."
      ],
      "metadata": {
        "id": "9O6MFtPsJQJF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nettoyage des données"
      ],
      "metadata": {
        "id": "sx2RD7IlJYcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# On regarde si les features de X_train_encode contiennent des NaN dans le pays qui leur est associé.\n",
        "# Tout d'abord, on regarde quelles features contiennent des NaN\n",
        "\n",
        "print(X_train_encode.isna().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJ4ZNLlJJIif",
        "outputId": "60d951bc-624f-4bf1-dcfa-85b2f02c8614"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID                    0\n",
            "DAY_ID                0\n",
            "DE_CONSUMPTION        0\n",
            "FR_CONSUMPTION        0\n",
            "DE_FR_EXCHANGE       25\n",
            "FR_DE_EXCHANGE       25\n",
            "DE_NET_EXPORT       124\n",
            "FR_NET_EXPORT        70\n",
            "DE_NET_IMPORT       124\n",
            "FR_NET_IMPORT        70\n",
            "DE_GAS                0\n",
            "FR_GAS                0\n",
            "DE_COAL               0\n",
            "FR_COAL               0\n",
            "DE_HYDRO              0\n",
            "FR_HYDRO              0\n",
            "DE_NUCLEAR            0\n",
            "FR_NUCLEAR            0\n",
            "DE_SOLAR              0\n",
            "FR_SOLAR              0\n",
            "DE_WINDPOW            0\n",
            "FR_WINDPOW            0\n",
            "DE_LIGNITE            0\n",
            "DE_RESIDUAL_LOAD      0\n",
            "FR_RESIDUAL_LOAD      0\n",
            "DE_RAIN              94\n",
            "FR_RAIN              94\n",
            "DE_WIND              94\n",
            "FR_WIND              94\n",
            "DE_TEMP              94\n",
            "FR_TEMP              94\n",
            "GAS_RET               0\n",
            "COAL_RET              0\n",
            "CARBON_RET            0\n",
            "COUNTRY_FR            0\n",
            "COUNTRY_DE            0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--> on voit que la colonne FR_GAS et DE_GAS contiennent 2 fois les mêmes valeurs pour chaque DAY_ID.\n",
        "En fait, c'est le cas pour toutes mes variables de mesure de production d'énergie et de mesures météorologiques."
      ],
      "metadata": {
        "id": "59bND0M3Umv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il semblerait que les variables de mesure de production d'énergie ont tout le temps la même valeur pour un même jour, peu importe s'il s'agit d'une ligne Allemagne ou d'une ligne France. Remarque : elles ne contiennent aucun NaN."
      ],
      "metadata": {
        "id": "KcylgPC5cNUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "De même, il semblerait que les variables de mesure météorologiques ont tout le temps la même valeur pour un même jour, peu importe s'il s'agit d'une ligne Allemagne ou d'une ligne France. Remarque : elles ne sont pas complètes (elles contiennent des NaN, et d'ailleurs ces NaN sont situés aux mêmes lignes)"
      ],
      "metadata": {
        "id": "0yoP2IzGcT_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Je veux vérifier que pour toutes les lignes qui ont le même DAY_ID, les valeurs de DE_RAIN sont les mêmes.\n",
        "Puis je ferai la même chose avec toutes mes autres variables météorologiques, puis avec toutes mes variables de mesure de production d'énergie"
      ],
      "metadata": {
        "id": "i9sK_fPHcY_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Afin de vérifier sur mes données TRAIN et TEST si les valeurs des variables météorologiques et de mesure de production sont les mêmes pour un même jour (peu importe s'il s'agit de la ligne Allemagne ou France), je concatène X_train et X_test, et je fais mes vérifications là-dessus"
      ],
      "metadata": {
        "id": "UDjZxZj9cgGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_all = pd.concat([X_train, X_test], ignore_index=True).sort_values([\"COUNTRY\", \"DAY_ID\"])"
      ],
      "metadata": {
        "id": "MnJr491ucj8Y"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = [\"DE_RAIN\", \"DE_WIND\", \"DE_TEMP\", \"FR_RAIN\", \"FR_WIND\", \"FR_TEMP\", \"DE_GAS\", \"DE_COAL\", \"DE_HYDRO\", \"DE_NUCLEAR\", \"DE_SOLAR\", \"DE_WINDPOW\",\n",
        "            \"FR_GAS\", \"FR_COAL\", \"FR_HYDRO\", \"FR_NUCLEAR\", \"FR_SOLAR\", \"FR_WINDPOW\", \"DE_CONSUMPTION\", \"DE_FR_EXCHANGE\", \"DE_NET_EXPORT\", \"DE_NET_IMPORT\",\n",
        "            \"DE_RESIDUAL_LOAD\", \"FR_CONSUMPTION\", \"FR_DE_EXCHANGE\", \"FR_NET_EXPORT\", \"FR_NET_IMPORT\", \"FR_RESIDUAL_LOAD\", \"CARBON_RET\", \"GAS_RET\", \"COAL_RET\"]\n",
        "results = {}\n",
        "for feature in features:\n",
        "  variances = X_all.groupby(\"DAY_ID\")[feature].nunique()\n",
        "  identiques = (variances <=1).all()\n",
        "  results[feature] = identiques\n",
        "\n",
        "results_df = pd.DataFrame.from_dict(results, orient=\"index\", columns=[\"Constante_par_jour\"])\n",
        "print(results_df)\n",
        "\n"
      ],
      "metadata": {
        "id": "_lhy119ZRwXT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8980c3d6-bd7f-417e-fe1d-706b3309de36"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  Constante_par_jour\n",
            "DE_RAIN                         True\n",
            "DE_WIND                         True\n",
            "DE_TEMP                         True\n",
            "FR_RAIN                         True\n",
            "FR_WIND                         True\n",
            "FR_TEMP                         True\n",
            "DE_GAS                          True\n",
            "DE_COAL                         True\n",
            "DE_HYDRO                        True\n",
            "DE_NUCLEAR                      True\n",
            "DE_SOLAR                        True\n",
            "DE_WINDPOW                      True\n",
            "FR_GAS                          True\n",
            "FR_COAL                         True\n",
            "FR_HYDRO                        True\n",
            "FR_NUCLEAR                      True\n",
            "FR_SOLAR                        True\n",
            "FR_WINDPOW                      True\n",
            "DE_CONSUMPTION                  True\n",
            "DE_FR_EXCHANGE                  True\n",
            "DE_NET_EXPORT                   True\n",
            "DE_NET_IMPORT                   True\n",
            "DE_RESIDUAL_LOAD                True\n",
            "FR_CONSUMPTION                  True\n",
            "FR_DE_EXCHANGE                  True\n",
            "FR_NET_EXPORT                   True\n",
            "FR_NET_IMPORT                   True\n",
            "FR_RESIDUAL_LOAD                True\n",
            "CARBON_RET                      True\n",
            "GAS_RET                         True\n",
            "COAL_RET                        True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "J'ai vérifié que toutes mes features (excepté ID, DAY_ID et COUNTRY) dans X_train et X_test ont les mêmes valeurs pour un même DAY_ID (peu importe si on est à une ligne Allemagne ou France)."
      ],
      "metadata": {
        "id": "P5FNCUc8bFQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maintenant, je vais compléter les NaN pour :\n",
        "- les variables météorologiques FR_TEMP, DE_WIND, FR_WIND, DE_TEMP, DE_RAIN, FR_RAIN, en interpolant temporellement.\n",
        "- les variables de mesure de production DE_NET_IMPORT, DE_NET_EXPORT, FR_NET_IMPORT, FR_NET_EXPORT, DE_FR_EXCHANGE, FR_DE_EXCHANGE. L'idée est de remplir les NaN avec une interpolation linéaire locale pour les trous courts, avec une moyenne mobile (rolling mean) pour lisser les périodes plus longues, et avec une propagation avant/après pour combler les bords éventuels."
      ],
      "metadata": {
        "id": "YDymGjzBbnNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imputation des NaN de mes variables météorologiques par interpolation temporelle\n",
        "cols_meteo = [\"FR_TEMP\", \"DE_WIND\", \"FR_WIND\", \"DE_TEMP\", \"DE_RAIN\", \"FR_RAIN\"]\n",
        "\n",
        "meteo_by_day = X_all.sort_values(\"DAY_ID\").groupby(\"DAY_ID\")[cols_meteo].mean() # on prend la moyenne des 2 valeurs d'une feature le même jour (les 2 valeurs sont égales sauf s'il y\n",
        "                                                                                # a une ligne avec un NaN et l'autre avec une valeur), ça renvoie un dataset indexé sur les DAY_ID croissants\n",
        "# print(meteo_by_day)\n",
        "\n",
        "meteo_interp = (meteo_by_day.interpolate(methode='linear', limit_direction='both')) # interpolation temporelle linéaire\n",
        "X_all = X_all.drop(columns=cols_meteo).merge(meteo_interp.reset_index(), on=\"DAY_ID\", how=\"left\")\n",
        "# print(X_all.head())\n",
        "\n",
        "# print(X_all.isna().sum()) #--> j'ai bien imputé les NaN de mes colonnes météo\n"
      ],
      "metadata": {
        "id": "Fm616Zp6Prk2"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imputation des NaN de mes variables de mesure de production\n",
        "\n",
        "cols_usage = [\n",
        "    \"DE_NET_IMPORT\", \"DE_NET_EXPORT\",\n",
        "    \"FR_NET_IMPORT\", \"FR_NET_EXPORT\",\n",
        "    \"DE_FR_EXCHANGE\", \"FR_DE_EXCHANGE\"\n",
        "]\n",
        "\n",
        "usage_by_day = X_all.sort_values(\"DAY_ID\").groupby(\"DAY_ID\")[cols_usage].mean()\n",
        "# print(usage_by_day)\n",
        "usage_interp = usage_by_day.copy()\n",
        "# imputation des valeurs NaN dans usage_by_day :\n",
        "  # interpolation linéaire locale pour les trous courts\n",
        "usage_interp = usage_interp.interpolate(method=\"linear\", limit=3, limit_direction=\"both\")\n",
        "  # moyenne mobile pour lisser les périodes les plus longues\n",
        "usage_interp = usage_interp.fillna(usage_interp.rolling(window=3, min_periods=1, center=True).mean())\n",
        "  # propagation avant après pour combler les bords éventuels\n",
        "usage_interp = usage_interp.ffill().bfill()\n",
        "\n",
        "X_all = X_all.drop(columns=cols_usage).merge(usage_interp.reset_index(), on=\"DAY_ID\", how=\"left\")\n",
        "\n",
        "# print(X_all.isna().sum())\n"
      ],
      "metadata": {
        "id": "84HMVfDDkVSD"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "J'ai mon X_all complet (sans NaN), maintenant je vais OneHotEncoder ma variable catégorielle COUNTRY"
      ],
      "metadata": {
        "id": "zTa7HTw_p_Qz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_all_encode = X_all.copy()\n",
        "\n",
        "X_all_encode[\"COUNTRY_DE\"] = (X_all_encode[\"COUNTRY\"] == \"DE\").astype(int)\n",
        "X_all_encode[\"COUNTRY_FR\"] = (X_all_encode[\"COUNTRY\"] == \"FR\").astype(int)\n",
        "\n",
        "X_all_encode = X_all_encode.drop(columns = \"COUNTRY\")\n",
        "\n",
        "# print(X_all_encode.head())"
      ],
      "metadata": {
        "id": "RaHVKSLAmaFI"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "J'ai mon X_all complet (sans NaN) et avec que des variables qui sont des int ou des float grâce au OneHotEncoding."
      ],
      "metadata": {
        "id": "0U8fzP9_q-K4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Désormais, je veux créer des lags sur mes variables. Pour choisir les lags pour chaque variable, je dois regarder pour chaque variable pour quels lags la corrélation croisée est la plus forte.\n",
        "Dans un premier temps, je fusionne mes TARGET avec mon dataset X_all, et j'appelle le tout **train_all**."
      ],
      "metadata": {
        "id": "gad2dG-yrV9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_all = X_all_encode.merge(y_train[[\"ID\", \"TARGET\"]], on=\"ID\", how=\"left\")\n",
        "#print(train_all.head())"
      ],
      "metadata": {
        "id": "uor808c4q1ze"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Objectif de cette cellule de code : renvoyer, pour chaque feature et chaque pays, les top_k lags (top_k_xcorr_all_features) par Spearman jusqu'à max_lag\n",
        "\n",
        "def _daily_series(df, col):\n",
        "  # renvoie une série indexée par DAY_ID (comme les valeurs sont les mêmes pour le DAY_ID, ça se fait facilement en agregeant par mean)\n",
        "  s = (df.groupby(\"DAY_ID\")[col].mean().sort_index())\n",
        "  return s\n",
        "\n",
        "\n",
        "def _daily_target_by_country(df, country_code, target_col=\"TARGET\"):\n",
        "  # renvoie la TARGET du pays demandé, indexée par DAY_ID, donc on filtre par pays\n",
        "  if country_code == \"DE\":\n",
        "    d = df[df[\"COUNTRY_DE\"] == 1]\n",
        "  elif country_code == \"FR\":\n",
        "    d = df[df[\"COUNTRY_FR\"] == 1]\n",
        "  y = (d.groupby(\"DAY_ID\")[target_col].mean().sort_index())\n",
        "  return y\n",
        "\n",
        "\n",
        "def xcorr_feature_target(df, feature, country_code, target_col=\"TARGET\", max_lag=5, method=\"spearman\", min_overlap=20):\n",
        "  # Calcule la corrélation entre Target_country(t) et Feature (t-k) pour k=0..max_lag.\n",
        "  # Retourne un DataFrame : lag, corr, abs_corr, n_overlap\n",
        "  x = _daily_series(df, feature) # série quotidienne unique du feature\n",
        "  y = _daily_target_by_country(df, country_code, target_col) # série quotidienne unique de la target par pays\n",
        "  idx = x.index.intersection(y.index) # aligne sur l'intersection des jours\n",
        "  x = x.loc[idx]\n",
        "  y = y.loc[idx]\n",
        "\n",
        "  rows=[]\n",
        "  for k in range(0, max_lag+1):\n",
        "    x_lag=x.shift(k)\n",
        "    aligned = pd.concat([y, x_lag], axis=1, keys=[\"y\", \"x_lag\"]).dropna() # on concatène les features laggées et la target\n",
        "    n = len(aligned)\n",
        "    if n < min_overlap:\n",
        "      rows.append({\"lag\": k, \"corr\": np.nan, \"abs_corr\": np.nan, \"n_overlap\": n})\n",
        "      continue\n",
        "    corr = aligned[\"y\"].corr(aligned[\"x_lag\"], method=method)\n",
        "    rows.append({\"lag\": k, \"corr\": corr, \"abs_corr\": abs(corr), \"n_overlap\": n})\n",
        "\n",
        "  out=pd.DataFrame(rows)\n",
        "  out[\"feature\"]=feature\n",
        "  out[\"country\"]=country_code\n",
        "  return out[[\"country\", \"feature\", \"lag\", \"corr\", \"abs_corr\", \"n_overlap\"]]\n",
        "\n",
        "\n",
        "\n",
        "def top_k_xcorr_all_features(df, features, countries=(\"DE\",\"FR\"), target_col=\"TARGET\", max_lag=30, top_k=3, method=\"spearman\", min_overlap=20):\n",
        "  # Pour chaque (pays, feature), calcule la corrélation sur les lags (entre 0 et max_lag), et renvoie les top_k par corrélation\n",
        "\n",
        "  all_rows=[]\n",
        "  for c in countries:\n",
        "    for f in features:\n",
        "      res = xcorr_feature_target(df, feature=f, country_code=c, target_col=target_col, max_lag=max_lag, method=method, min_overlap=min_overlap)\n",
        "      # garde les top_k lags par |ρ|\n",
        "      res = res.sort_values(\"abs_corr\", ascending=False).head(top_k)\n",
        "      all_rows.append(res)\n",
        "\n",
        "  out = pd.concat(all_rows, ignore_index=True)\n",
        "  out = out.sort_values([\"country\", \"abs_corr\", \"feature\"], ascending=[True, False, True])\n",
        "  return out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I14txzJCsKbJ"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# je teste mon code\n",
        "\n",
        "features = [\"DE_RAIN\", \"DE_WIND\", \"DE_TEMP\", \"FR_RAIN\", \"FR_WIND\", \"FR_TEMP\", \"DE_GAS\", \"DE_COAL\", \"DE_HYDRO\", \"DE_NUCLEAR\", \"DE_SOLAR\", \"DE_WINDPOW\",\n",
        "            \"FR_GAS\", \"FR_COAL\", \"FR_HYDRO\", \"FR_NUCLEAR\", \"FR_SOLAR\", \"FR_WINDPOW\", \"DE_CONSUMPTION\", \"DE_FR_EXCHANGE\", \"DE_NET_EXPORT\", \"DE_NET_IMPORT\",\n",
        "            \"DE_RESIDUAL_LOAD\", \"FR_CONSUMPTION\", \"FR_DE_EXCHANGE\", \"FR_NET_EXPORT\", \"FR_NET_IMPORT\", \"FR_RESIDUAL_LOAD\", \"CARBON_RET\", \"GAS_RET\", \"COAL_RET\"]\n",
        "print(len(features))\n",
        "res = top_k_xcorr_all_features(train_all, features, countries=(\"DE\",\"FR\"), target_col=\"TARGET\", max_lag=5, top_k=3, method=\"spearman\", min_overlap=20)\n",
        "#print(res.shape)\n",
        "print(res.head(10))\n",
        "\n",
        "\n",
        "subset = res[(res[\"country\"] == \"DE\") & (res[\"feature\"] == \"DE_GAS\")] \\\n",
        "            .sort_values(\"abs_corr\", ascending=False) \\\n",
        "            [[\"lag\", \"corr\", \"abs_corr\", \"n_overlap\"]]\n",
        "\n",
        "print(subset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BDuIBhZWuNx",
        "outputId": "0f12ed9e-cea5-4aa4-9045-8704c1856c1b"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31\n",
            "   country           feature  lag      corr  abs_corr  n_overlap\n",
            "66      DE  DE_RESIDUAL_LOAD    0  0.324335  0.324335        643\n",
            "60      DE     DE_NET_EXPORT    0 -0.306204  0.306204        643\n",
            "63      DE     DE_NET_IMPORT    0  0.306204  0.306204        643\n",
            "33      DE        DE_WINDPOW    0 -0.300933  0.300933        643\n",
            "18      DE            DE_GAS    0  0.253410  0.253410        643\n",
            "24      DE          DE_HYDRO    0  0.217900  0.217900        643\n",
            "51      DE        FR_WINDPOW    0 -0.199820  0.199820        643\n",
            "3       DE           DE_WIND    0 -0.146598  0.146598        643\n",
            "21      DE           DE_COAL    0  0.142054  0.142054        643\n",
            "25      DE          DE_HYDRO    5 -0.117970  0.117970        640\n",
            "    lag      corr  abs_corr  n_overlap\n",
            "18    0  0.253410  0.253410        643\n",
            "19    3 -0.077079  0.077079        641\n",
            "20    4 -0.016348  0.016348        640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Création du dataset avec les lags (**train_all_lag**)"
      ],
      "metadata": {
        "id": "e5mZxHR8rWd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Pour chaque couple (pays, feature), on prend le lag max trouvé dans res\n",
        "- On crée les colonnes t-1 à t-L : FEATURE_COUNTRY_1, …, FEATURE_COUNTRY_L\n",
        "- On masque ces colonnes (met NaN) sur les lignes qui ne correspondent pas au pays"
      ],
      "metadata": {
        "id": "NxD4VG54rgQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_maxlag_map(res_df):\n",
        "  \"\"\" A partir de 'res' (top_k par (country, feature)), construit un dict { (country, feature) : max_lag_trouve }.\"\"\"\n",
        "  # on ignore les lags NaN au cas où\n",
        "  valid = res_df.dropna(subset=[\"lag\"])\n",
        "  maxlag = valid.groupby([\"country\", \"feature\"])[\"lag\"].max()\n",
        "  return maxlag.to_dict()\n",
        "\n",
        "def add_lag_columns_for_pair(df, feature, country_code, max_lag, day_col=\"DAY_ID\"):\n",
        "  \"\"\"Ajoute au DataFrame 'df' les colonnes laggées pour un couple (feature, country_code),\n",
        "     depuis t-1 jusqu'à t-max_lag. Masque ensuite les lignes du pays non concerné en NaN.\n",
        "     Noms de colonnes créés: f\"{feature}_{country_code}_{k}\" pour k=1..max_lag\"\"\"\n",
        "\n",
        "  if max_lag <= 0:\n",
        "    return df\n",
        "  daily = df.groupby(day_col)[feature].mean().sort_index()\n",
        "  # On construit un petit DataFrame de lags\n",
        "  lag_cols = {}\n",
        "  for k in range(1, max_lag + 1):\n",
        "    lag_cols[f\"{feature}_{country_code}_{k}\"] = daily.shift(k)\n",
        "  lag_df = pd.DataFrame(lag_cols, index=daily.index).reset_index()\n",
        "\n",
        "  # On fusionne sur DAY_ID (donc on fusionne même sur les lignes qui ne vérifient pas le country_code,\n",
        "  # mais c'est pas grave car on remplira ces lignes par des NaN plus tard)\n",
        "  df = df.merge(lag_df, on=day_col, how=\"left\")\n",
        "\n",
        "  # On remplit les lignes dont le pays n'est pas country_code par NaN\n",
        "\n",
        "  mask_col = \"COUNTRY_DE\" if country_code == \"DE\" else \"COUNTRY_FR\"\n",
        "  new_cols = [f\"{feature}_{country_code}_{k}\" for k in range(1, max_lag + 1)]\n",
        "  df.loc[df[mask_col] != 1, new_cols] = np.nan\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "\n",
        "def build_train_all_lag(train_all, res, day_col=\"DAY_ID\"):\n",
        "  \"\"\"Construit train_all_lag en ajoutant toutes les colonnes laggées\n",
        "     pour chaque (pays, feature) jusqu'au lag maximum trouvé dans 'res'.\"\"\"\n",
        "\n",
        "  train_all_lag = train_all.copy()\n",
        "  maxlag_map = build_maxlag_map(res)\n",
        "\n",
        "  for (country_code, feature), L in maxlag_map.items():\n",
        "    if feature not in train_all_lag.columns:\n",
        "      print(f\"[AVERTISSEMENT] Colonne '{feature}' absente, sautée.\")\n",
        "      continue\n",
        "    train_all_lag = add_lag_columns_for_pair(train_all_lag, feature=feature, country_code=country_code, max_lag=L, day_col=day_col)\n",
        "\n",
        "  return train_all_lag"
      ],
      "metadata": {
        "id": "SpgUEGIWdqqk"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# je teste mon code\n",
        "\n",
        "train_all_lag = build_train_all_lag(train_all, res, day_col=\"DAY_ID\")\n",
        "print([c for c in train_all_lag.columns if c.startswith(\"DE_GAS_DE_\")][:10])\n",
        "print(train_all_lag.shape)\n",
        "print(train_all_lag.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWPbPaUqxoxl",
        "outputId": "a3d1a310-9fd3-42e7-aa3d-41cfd09b4521"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['DE_GAS_DE_1', 'DE_GAS_DE_2', 'DE_GAS_DE_3', 'DE_GAS_DE_4']\n",
            "(2148, 309)\n",
            "    ID  DAY_ID  DE_CONSUMPTION  FR_CONSUMPTION    DE_GAS    FR_GAS   DE_COAL  \\\n",
            "0  111       2       -0.068972       -0.667390  1.228079  0.458302 -0.247704   \n",
            "1  800       3       -0.134670       -0.834564  1.588542  0.069297 -0.635452   \n",
            "2  410       4        0.944117        0.203547  0.451085  0.865857 -0.752504   \n",
            "3  831       5       -0.297850       -0.470371  1.059828  0.528273 -0.072071   \n",
            "4  562       6        0.120788       -0.744840  1.524895 -0.395402 -0.079164   \n",
            "\n",
            "    FR_COAL  DE_HYDRO  FR_HYDRO  ...  FR_TEMP_FR_3  FR_TEMP_FR_4  \\\n",
            "0 -0.766904  1.785758 -0.930172  ...           NaN           NaN   \n",
            "1 -0.718729  1.994144 -0.383690  ...           NaN           NaN   \n",
            "2 -0.782998 -0.291237 -0.170210  ...           NaN           NaN   \n",
            "3 -0.766063  1.275857 -0.398178  ...           NaN           NaN   \n",
            "4 -0.794950  2.327561  0.074597  ...           NaN           NaN   \n",
            "\n",
            "   FR_WIND_FR_1  FR_WIND_FR_2  FR_WINDPOW_FR_1  FR_WINDPOW_FR_2  \\\n",
            "0           NaN           NaN              NaN              NaN   \n",
            "1           NaN           NaN              NaN              NaN   \n",
            "2           NaN           NaN              NaN              NaN   \n",
            "3           NaN           NaN              NaN              NaN   \n",
            "4           NaN           NaN              NaN              NaN   \n",
            "\n",
            "   FR_WINDPOW_FR_3  GAS_RET_FR_1  GAS_RET_FR_2  GAS_RET_FR_3  \n",
            "0              NaN           NaN           NaN           NaN  \n",
            "1              NaN           NaN           NaN           NaN  \n",
            "2              NaN           NaN           NaN           NaN  \n",
            "3              NaN           NaN           NaN           NaN  \n",
            "4              NaN           NaN           NaN           NaN  \n",
            "\n",
            "[5 rows x 309 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Désormais mon train_all_lag contient 309 colonnes."
      ],
      "metadata": {
        "id": "_TE1k6db0mW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ajout des valeurs de prédiction (des leads bruités) dans mon dataset"
      ],
      "metadata": {
        "id": "uPvXTI_c39cF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mais je peux considérer que les valeurs des futures au jour t dépendent des prédictions météorologiques des jours suivants non? Or les personnes chargées de spéculer sur les valeurs de ces futures disposent de modèles puissants pour anticiper les prédictions météorologiques des jours suivants. Mais moi je ne dispose que des mesures météorologiques des jours précédents et des jours suivants, donc comme je dois estimer ce que prédirait un trader pour les valeurs des futures des jours suivants, je peux simuler que je dispose de modèles météorologiques puissants en utilisant les données météorologiques des jours suivants que je bruite afin de faire des prédictions non ?\n",
        "\n",
        "Que penses-tu de ce raisonnement ?"
      ],
      "metadata": {
        "id": "roTpJubp83y4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AMBrLHj_83eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W4N9kBLyybat"
      },
      "execution_count": 55,
      "outputs": []
    }
  ]
}