{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mNGZ4FHM4r9o",
        "HZGmdgm642Rc",
        "F29dsdLt5DHQ",
        "4gSR8tydJ_3f"
      ],
      "authorship_tag": "ABX9TyPrZS4da9SAErUYJiVCpMC8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Buzon-coder/QRT-Electricity-price/blob/main/QRT_%C3%A9lec_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1Azcp1ysD06",
        "outputId": "7d949cba-8428-41d7-8ca8-341c2dd0e845"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importation de mes bibliothèques"
      ],
      "metadata": {
        "id": "QHG2bgg2JP2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import spearmanr, pearsonr\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "9g-kMtbYsWjr"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chargement des fichiers csv"
      ],
      "metadata": {
        "id": "mNGZ4FHM4r9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = pd.read_csv(\"/content/drive/MyDrive/Colab data/X_test_final.csv\")\n",
        "X_train = pd.read_csv(\"/content/drive/MyDrive/Colab data/X_train_NHkHMNU.csv\")\n",
        "y_train = pd.read_csv(\"/content/drive/MyDrive/Colab data/y_train_ZAN5mwg.csv\")"
      ],
      "metadata": {
        "id": "TTP3pvQpse2T"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encodage de ma (seule) variable catégorielle COUNTRY"
      ],
      "metadata": {
        "id": "uHPn7pnS4vLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_encode = X_train.copy()\n",
        "X_train_encode[\"COUNTRY_FR\"] = (X_train_encode[\"COUNTRY\"] == \"FR\").astype(int)\n",
        "X_train_encode[\"COUNTRY_DE\"] = (X_train_encode[\"COUNTRY\"] == \"DE\").astype(int)\n",
        "X_train_encode = X_train_encode.drop(columns = [\"COUNTRY\"])"
      ],
      "metadata": {
        "id": "wkF65GulslnW"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calcul des corrélations de spearman entre mes features et mes valeurs cibles"
      ],
      "metadata": {
        "id": "HZGmdgm642Rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Je vérifie que X_train_encode et y_train ont les mêmes index\n",
        "print((X_train_encode.index == y_train.index).all())\n",
        "# c'est le cas, donc je peux utiliser la fonction corrwith pour calculer la corrélation entre les features de X_train_encode et y_train\n",
        "corrs = X_train_encode.corrwith(y_train[\"TARGET\"], method = 'spearman')\n",
        "corrs_abs = corrs.abs().sort_values(ascending=False)\n",
        "# print(corrs_abs.head(20))\n",
        "# print(corrs_abs.tail(18))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Nkn9EFk-NEU",
        "outputId": "f6f220f3-9b26-4d8a-d6ae-a76f6d46d8f2"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "J'obtiens que les features les plus corrélées (en valeur absolue) de X_train et avec les TARGET (de y_train) sont par ordre décroissant : DE_NET_IMPORT, DE_NET_EXPORT, DE_WINDPOW, DE_RESIDUAL_LOAD, FR_WINDPOW,DE_HYDRO, DE_GAS, CARBON_RET, DE_WIND, DE_COAL, GAS_RET, FR_HYDRO,FR_WIND, DE_CONSUMPTION, FR_RAIN, FR_DE_EXCHANGE, DE_FR_EXCHANGE, FR_COAL, FR_TEMP, DE_LIGNITE.\n",
        "\\\n",
        "\n",
        "De même, j'obtiens les features les moins corrélées (en valeur absolue) de X_train avec les TARGET sont (de la plus corrélées à la moins corrélée) : DE_RAIN, DE_SOLAR, FR_SOLAR, FR_CONSUMPTION, ID, COUNTRY_FR, COUNTRY_DE, DE_TEMP, FR_RESIDUAL_LOAD, COAL_RET, FR_GAS, FR_NET_EXPORT, FR_NET_IMPORT, DE_NUCLEAR, DAY_ID, FR_NUCLEAR.\n",
        "\\\n",
        "\n",
        "Regarder les corrélations permet de comprendre quelles features influencent TARGET, et permet d'interpréter phyisquement le modèle. Ces features représentent les effets dominants."
      ],
      "metadata": {
        "id": "ig32deUr-pXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing des features"
      ],
      "metadata": {
        "id": "rIKbMt1J4_tS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# a) Détection des NaN"
      ],
      "metadata": {
        "id": "F29dsdLt5DHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print(X_train_encode.isna().sum())\n",
        "# print(X_test.isna().sum())"
      ],
      "metadata": {
        "id": "wlJJSoGZs-k9"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mes 12 colonnes qui comportent des NaN sont les mêmes dans X_test et X_train, et ce sont :\n",
        "- toutes mes variables météorologiques\n",
        "- mes variables de mesure d'utilisation électrique journalière (x_NET_IMPORT,x_NET_EXPORT, x_y_EXCHANGE)."
      ],
      "metadata": {
        "id": "o5kVkeCe6Flc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " # b) Imputation des NaN (on obtient X_imputed)"
      ],
      "metadata": {
        "id": "-P3YBwM1CupA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Je vais faire une imputation par régression (IterativeImputer()), c'est-à-dire entraîner un petit modèle pour chaque variable manquante (x_TEMP) à partir des autres features corrélées (x_RAIN, x_WIND).\n",
        "\\\n",
        "\n",
        "L'idée de IterativeImputer() pour imputer les NaN de la colonne X_j, c'est d'utiliser toutes les autres colonnes (tous les X_k avec k != j) comme features explicatives, puis d'entraîner un modèle de régression sur les lignes où X_j n'est pas manquant.\n",
        "\\\n",
        "\n",
        "Remarque : Il faudra faire attention, car j'ai rempli des lignes correspondant à DE dans des colonnes sur des données FR, et réciproquement."
      ],
      "metadata": {
        "id": "ZV5ssBcCCyhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imputer = IterativeImputer()\n",
        "X_imputed = imputer.fit_transform(X_train_encode)\n",
        "X_imputed = pd.DataFrame(X_imputed, columns=X_train_encode.columns, index=X_train_encode.index) # la fonction IterativeImputer renvoie un numpy.ndarray\n",
        "                                                                                                # donc je le retransforme en DataFrame ensuite\n",
        "\n",
        "# print(type(X_imputed))\n",
        "# print(X_imputed.isna().sum()) # --> toutes mes valeurs NaN ont bien été imputées"
      ],
      "metadata": {
        "id": "nkn_VeDh5KrQ"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# c) Vérification de la multicolinéarité entre les features"
      ],
      "metadata": {
        "id": "4gSR8tydJ_3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La VIF (variance inflation factor, facteur d'inflation de la variance) d'une variable X_i mesure à quel point elle peut être expliquée par les autres variables du dataset."
      ],
      "metadata": {
        "id": "lbKn6Fv5KFov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vif = pd.DataFrame()\n",
        "vif[\"feature\"] = X_imputed.columns\n",
        "vif[\"VIF\"] = [variance_inflation_factor(X_imputed.values, i) for i in range(X_imputed.shape[1])]\n",
        "\n",
        "high_vif_features = vif.loc[vif[\"VIF\"] >=10, \"feature\"].tolist()\n",
        "print(high_vif_features)\n",
        "print(len(high_vif_features))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BU1y0cFDDg0D",
        "outputId": "7c4e8094-431a-46b2-9fbd-2f42bc8e347c"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ID', 'DE_CONSUMPTION', 'FR_CONSUMPTION', 'DE_FR_EXCHANGE', 'FR_DE_EXCHANGE', 'DE_NET_EXPORT', 'FR_NET_EXPORT', 'DE_NET_IMPORT', 'FR_NET_IMPORT', 'FR_GAS', 'FR_NUCLEAR', 'DE_SOLAR', 'FR_SOLAR', 'DE_WINDPOW', 'FR_WINDPOW', 'DE_RESIDUAL_LOAD', 'FR_RESIDUAL_LOAD', 'COUNTRY_FR']\n",
            "18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "J'ai 18 features qui ont un facteur d'inflation de la variance >= 10, donc j'ai beaucoup de multicolinéarité, d'où l'intérêt d'utiliser un algorithme avec pénalisation.\n",
        "\n",
        "\\\n",
        "\n",
        "Plus tard, je vais déterminer quelles sont les features les plus utiles avec Ridge et Lasso."
      ],
      "metadata": {
        "id": "6ujwxbmgNZRC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# d) Standardistation des features (on obtient X_scaled)"
      ],
      "metadata": {
        "id": "eOHhcBnOSJXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_imputed)\n",
        "X_scaled = pd.DataFrame(X_scaled, columns = X_imputed.columns, index = X_imputed.index)\n",
        "# print(X_scaled[\"FR_GAS\"].describe())\n",
        "# print(X_scaled[\"FR_DE_EXCHANGE\"].describe())"
      ],
      "metadata": {
        "id": "UcqIbGv2KU6B"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En fait à refaire pour faire gaffe aux lignes DE dans les colonnes FR.\n",
        "prompt ChatGPT :\n",
        "\n",
        "J'ai fait ça : imputer = IterativeImputer() X_imputed = imputer.fit_transform(X_train_encode) X_imputed = pd.DataFrame(X_imputed, columns=X_train_encode.columns, index=X_train_encode.index) Mais en fait, je veux que deux lignes d'une même colonne qui ont le même DAY_ID aient la même valeur, et que cette valeur soit choisie en fonction de la colonne. S'il s'agit de la colonne FR_RAIN, je veux que la valeur du NaN aux lignes COUNTRY_DE == 1 soit la même que la valeur obtenue par IterativeImputer() à la ligne COUNTRY_FR == 1 pour un même DAY_ID. Donc en fait, ce serait peut-être bien de faire un dataset pour les features françaises (FR_RAIN, FR_WIND, FR_TEMP, FR_NET_IMPORT, FR_NET_EXPORT) dans lequel on ne met que les lignes qui ont COUNTRY_FR == 1 et de faire le IterativeImputer() sur ce dataset uniquement pour ces features, et de même de faire un dataset pour les features allemandes (DE_RAIN, DE_WIND, DE_TEMP, DE_NET_IMPORT, DE_NET_EXPORT) et de faire le IterativeImputer() sur ce dataset uniquement pour ces features, qu'en penses-tu ? Ensuite, il me manquera les colonnes DE_FR_EXCHANGE et FR_DE_EXCHANGE à imputer, et comme les valeurs de DE_FR_EXCHANGE sont exactement l'opposée des valeurs de FR_DE_EXCHANGE, je n'aurai besoin que de déduire les valeurs à imputer aux NaN d'une colonne pour déduire celles à imputer à l'autre. Enfin, j'aimerais réunir mes deux datasets (un par pays) dont j'ai imputé les NaN en un seul. Est-ce que tu vois un problème dans mon raisonnement ?\n",
        "\n",
        "\n",
        "\n",
        "Mais pour imputer des valeurs NaN dans ma colonne FR_NET_IMPORT par exemple, je veux utiliser toutes les colonnes de mon dataset de départ, pas uniquement les colonnes fr_cols. Car elles vont toutes avoir une influence directe ou indirecte sur la valeur à imputer. Par contre, je ne veux me baser que sur les lignes qui vérifient COUNTRY_FR == 1. J'ai l'impression que dans ton code tu as enlevé les colonnes autres que fr_cols pour mon dataset français, et les colonnes autres que de_cols pour mon dataset allemand, est-ce que je me trompe? Moi je ne veux pas enlever ces colonnes, ce que je veux, c'est utiliser les lignes adéquat, donc faire un groupby country pour obtenir chaque dataset.\n",
        "\n",
        "\n",
        "\n",
        "regarder chat ML 2.4.2 (QR élec), normalement il m'a fait le code"
      ],
      "metadata": {
        "id": "cBG2_PrEpJGg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mes features sont bien centrées en 0 (moyenne = 0) et d'écart-type égal à 1."
      ],
      "metadata": {
        "id": "XQb2cT3tdls2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# e) Feature Engineering"
      ],
      "metadata": {
        "id": "ZCvRw_xpSSb7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pUlKbeu_SVQ0"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# f) Réduction de dimension (optionnel)"
      ],
      "metadata": {
        "id": "KhulMo95SXOy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wc5Lvuh3SaYr"
      },
      "execution_count": 67,
      "outputs": []
    }
  ]
}